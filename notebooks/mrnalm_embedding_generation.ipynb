{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42469f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:04:25.736023Z",
     "iopub.status.busy": "2025-03-14T23:04:25.735756Z",
     "iopub.status.idle": "2025-03-14T23:04:30.339685Z",
     "shell.execute_reply": "2025-03-14T23:04:30.338544Z"
    },
    "papermill": {
     "duration": 4.610027,
     "end_time": "2025-03-14T23:04:30.341844",
     "exception": false,
     "start_time": "2025-03-14T23:04:25.731817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup directories and install dependencies\n",
    "!mkdir -p \"mrnalm_embeddings/joint_embs\"\n",
    "!mkdir -p \"mrnalm_embeddings/hidden_states\"\n",
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0db572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:04:52.482479Z",
     "iopub.status.busy": "2025-03-14T23:04:52.482013Z",
     "iopub.status.idle": "2025-03-14T23:04:52.486848Z",
     "shell.execute_reply": "2025-03-14T23:04:52.486143Z"
    },
    "papermill": {
     "duration": 0.009503,
     "end_time": "2025-03-14T23:04:52.488090",
     "exception": false,
     "start_time": "2025-03-14T23:04:52.478587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "lst_ele = list(\"AUGCN\")\n",
    "lst_voc = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "for a1 in lst_ele:\n",
    "    for a2 in lst_ele:\n",
    "        for a3 in lst_ele:\n",
    "            lst_voc.extend([f\"{a1}{a2}{a3}\"])\n",
    "n_indexes = [\n",
    "    i for i, item in enumerate(lst_voc) if (\"N\" in item) and (not item.startswith(\"[\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc27818d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:04:52.494758Z",
     "iopub.status.busy": "2025-03-14T23:04:52.494493Z",
     "iopub.status.idle": "2025-03-14T23:04:52.898027Z",
     "shell.execute_reply": "2025-03-14T23:04:52.897191Z"
    },
    "papermill": {
     "duration": 0.408632,
     "end_time": "2025-03-14T23:04:52.899615",
     "exception": false,
     "start_time": "2025-03-14T23:04:52.490983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mRNA-LM Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.activations import gelu\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import *\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "from transformers import BertForMaskedLM, PreTrainedTokenizerFast\n",
    "\n",
    "########### PEFT\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "\n",
    "class FullModel(torch.nn.Module):\n",
    "    def __init__(self, num_labels, class_weights, lorar, lalpha, ldropout, head_dim=768, head_droupout=0.5, useCLIP=False, temperature=0.07, clip_coeff=0.2):\n",
    "        super(FullModel, self).__init__()\n",
    "        \n",
    "        # tokenizer\n",
    "        self.tokenizer_cds = None\n",
    "        self.tokenizer_5utr = None\n",
    "        self.tokenizer_3utr = None\n",
    "        self.build_tokenizer()\n",
    "        self.CLIP = useCLIP\n",
    "        \n",
    "        # model \n",
    "        self.utr5 = BertForMaskedLM.from_pretrained(\"models/mrna_5utr_model/mrna_5utr_model_p2_cp85600_best\")\n",
    "        self.utr3 = BertForMaskedLM.from_pretrained(\"models/mrna_3utr_model/mrna_3utr_model_p2_cp99900_best\")\n",
    "        self.cds = BertForMaskedLM.from_pretrained(\"models/CodonBERT/codonbert\")\n",
    "        \n",
    "        # gradient_checkpointing_enable: trading speed for memory\n",
    "        # self.utr5.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "        # self.utr3.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "        # self.cds.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "        ########### lora\n",
    "        if lorar > 0:\n",
    "            peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS,\n",
    "                                    r=lorar, \n",
    "                                    lora_alpha=lalpha, \n",
    "                                    lora_dropout=ldropout,\n",
    "                                    use_rslora=True)\n",
    "\n",
    "            self.utr5 = get_peft_model(self.utr5, peft_config)\n",
    "            self.utr5.print_trainable_parameters()\n",
    "            # self.utr5.gradient_checkpointing_enable()\n",
    "            # self.utr5.enable_input_require_grads()\n",
    "\n",
    "            self.utr3 = get_peft_model(self.utr3, peft_config)\n",
    "            self.utr3.print_trainable_parameters()\n",
    "            # self.utr3.gradient_checkpointing_enable()\n",
    "            # self.utr3.enable_input_require_grads()\n",
    "\n",
    "            self.cds = get_peft_model(self.cds, peft_config)\n",
    "            self.cds.print_trainable_parameters()\n",
    "            # self.cds.gradient_checkpointing_enable()\n",
    "            # self.cds.enable_input_require_grads()\n",
    "            \n",
    "\n",
    "        # Dense layers for CLIP-style structure\n",
    "        self.dense_utr5 = nn.Linear(768, 768)\n",
    "        self.dense_cds1 = nn.Linear(768, 768)\n",
    "        self.dense_cds2 = nn.Linear(768, 768)\n",
    "        self.dense_utr3 = nn.Linear(768, 768)\n",
    "\n",
    "        self.final_dense = nn.Linear(768*3, head_dim)\n",
    "\n",
    "        self.transform_act_fn = gelu\n",
    "        self.LayerNorm = torch.nn.LayerNorm(head_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(head_droupout)\n",
    "\n",
    "        self.decoder = nn.Linear(head_dim, num_labels, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_labels))\n",
    "        self.decoder.bias = self.bias\n",
    "        \n",
    "        if num_labels == 1:\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            class_weights=torch.tensor(class_weights, dtype=torch.float)\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "        # Temperature for scaling logits\n",
    "        self.temperature = temperature\n",
    "        self.clip_coeff = clip_coeff\n",
    "        self.is_first_epoch = True\n",
    "        \n",
    "\n",
    "    def cross_entropy_loss(self, preds, targets, reduction='none'):\n",
    "        log_softmax_preds = log_softmax(preds, dim=-1)\n",
    "        loss = (-targets * log_softmax_preds).sum(1)\n",
    "        if reduction == \"none\":\n",
    "            return loss\n",
    "        elif reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        \n",
    "    def contrastive_loss(self, embeds1, embeds2):\n",
    "        # Normalize the embeddings\n",
    "        embeds1 = nn.functional.normalize(embeds1, p=2, dim=1)\n",
    "        embeds2 = nn.functional.normalize(embeds2, p=2, dim=1)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        logits = torch.matmul(embeds1, embeds2.t()) / self.temperature\n",
    "        similarity_1 = torch.matmul(embeds1, embeds1.t())\n",
    "        similarity_2 = torch.matmul(embeds2, embeds2.t())\n",
    "        \n",
    "        # Calculate targets\n",
    "        targets = softmax((similarity_1 + similarity_2) / 2 * self.temperature, dim=-1)\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        loss1 = self.cross_entropy_loss(logits, targets, reduction='none')\n",
    "        loss2 = self.cross_entropy_loss(logits.t(), targets.t(), reduction='none')\n",
    "        \n",
    "        return (loss1.mean() + loss2.mean()) / 2\n",
    "    \n",
    "    def combine_embeds(self, input_ids, attention_mask, model, model_max_seq_length):\n",
    "        # maximum length in the batch\n",
    "        seq_len = torch.sum(attention_mask, 1)\n",
    "        max_seq_length = torch.max(seq_len).item()\n",
    "\n",
    "        i = 0 \n",
    "        embeds = []\n",
    "        while i < max_seq_length:\n",
    "            features = {\"input_ids\": input_ids[:, i:min(max_seq_length, i+model_max_seq_length-2)], \"attention_mask\": attention_mask[:, i:min(max_seq_length, i+model_max_seq_length-2)]}\n",
    "            output_states = model(**features, output_hidden_states=True)\n",
    "            embeds.append(output_states[\"hidden_states\"][-1])\n",
    "            i += model_max_seq_length - 2\n",
    "        embeds = torch.cat(embeds, 1)\n",
    "        # print(embeds.size())\n",
    "\n",
    "        return embeds, attention_mask[:, :max_seq_length]\n",
    "\n",
    "    def get_mean_token_embeddings(self, token_embeddings, token_mask):\n",
    "        input_mask_expanded = token_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / sum_mask\n",
    "\n",
    "        return sum_embeddings\n",
    "\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3, labels, return_hidden=False, epoch=None, decay_rate=0.95, **kwargs):\n",
    "        utr5_embeds = self.utr5(input_ids=input_ids1, attention_mask=attention_mask1, output_hidden_states=True)[\"hidden_states\"][-1]\n",
    "        cds_embeds  = self.cds(input_ids=input_ids2, attention_mask=attention_mask2, output_hidden_states=True)[\"hidden_states\"][-1]\n",
    "        utr3_embeds = self.utr3(input_ids=input_ids3, attention_mask=attention_mask3, output_hidden_states=True)[\"hidden_states\"][-1]\n",
    "\n",
    "        utr5_sum_embeddings = self.get_mean_token_embeddings(utr5_embeds[:, 1:-1, :], attention_mask1[:, 1:-1])\n",
    "        cds_sum_embeddings  = self.get_mean_token_embeddings(cds_embeds[:, 1:-1, :], attention_mask2[:, 1:-1])\n",
    "        utr3_sum_embeddings = self.get_mean_token_embeddings(utr3_embeds[:, 1:-1, :], attention_mask3[:, 1:-1])\n",
    "\n",
    "        if not self.CLIP:\n",
    "            joint_embed = torch.cat([utr5_sum_embeddings, cds_sum_embeddings, utr3_sum_embeddings], dim=1)\n",
    "\n",
    "            hidden_states = self.final_dense(joint_embed)\n",
    "            hidden_states = self.transform_act_fn(hidden_states)\n",
    "            hidden_states = self.LayerNorm(hidden_states)\n",
    "\n",
    "            hidden_states = self.dropout(hidden_states)\n",
    "            logits = self.decoder(hidden_states).squeeze()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "            if not return_hidden:\n",
    "                return loss, logits\n",
    "\n",
    "            return joint_embed, hidden_states\n",
    "\n",
    "        # CLIP-style transformations\n",
    "        utr5_transformed = self.dense_utr5(utr5_sum_embeddings)\n",
    "        cds_transformed1 = self.dense_cds1(cds_sum_embeddings)\n",
    "        cds_transformed2 = self.dense_cds2(cds_sum_embeddings)\n",
    "        utr3_transformed = self.dense_utr3(utr3_sum_embeddings)\n",
    "\n",
    "        # Apply CLIP-style contrastive loss with no_grad to avoid affecting the main graph\n",
    "        clip_loss1 = self.contrastive_loss(utr5_transformed, cds_transformed1)\n",
    "        clip_loss2 = self.contrastive_loss(cds_transformed2, utr3_transformed)\n",
    "        average_clip_loss = (clip_loss1 + clip_loss2) / 2\n",
    "\n",
    "        # Combine the embeddings for the final classification task\n",
    "        combined_hidden_states = torch.cat([utr5_transformed, cds_transformed1, utr3_transformed], dim=1)\n",
    "\n",
    "        combined_hidden_states = self.final_dense(combined_hidden_states)\n",
    "        combined_hidden_states = self.transform_act_fn(combined_hidden_states)\n",
    "        combined_hidden_states = self.LayerNorm(combined_hidden_states)\n",
    "\n",
    "        combined_hidden_states = self.dropout(combined_hidden_states)\n",
    "        logits = self.decoder(combined_hidden_states).squeeze()\n",
    "        classification_loss = self.loss_fn(logits, labels) \n",
    "\n",
    "        # Initialize clip_coeff if first epoch\n",
    "        if self.is_first_epoch:\n",
    "            self.is_first_epoch = False\n",
    "            self.clip_coeff = classification_loss.item() / average_clip_loss.item() * self.clip_coeff\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = classification_loss + average_clip_loss * self.clip_coeff\n",
    "\n",
    "        return total_loss, logits\n",
    "\n",
    "\n",
    "    def compute_contrastive_loss(self, utr5_proj, cds_proj, utr3_proj, temperature=0.07):\n",
    "        # Normalize the projections\n",
    "        utr5_proj_norm = F.normalize(utr5_proj, dim=-1)\n",
    "        cds_proj_norm = F.normalize(cds_proj, dim=-1)\n",
    "        utr3_proj_norm = F.normalize(utr3_proj, dim=-1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(utr5_proj_norm, cds_proj_norm.T) / temperature\n",
    "        \n",
    "        # Labels for contrastive loss\n",
    "        labels = torch.arange(similarity_matrix.size(0)).to(similarity_matrix.device)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        contrastive_loss = loss_fct(similarity_matrix, labels) + loss_fct(similarity_matrix.T, labels)\n",
    "        \n",
    "        return contrastive_loss\n",
    "    \n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        lst_ele = list('AUGCN')\n",
    "        lst_voc = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        for a1 in lst_ele:\n",
    "            for a2 in lst_ele:\n",
    "                for a3 in lst_ele:\n",
    "                    lst_voc.extend([f'{a1}{a2}{a3}'])\n",
    "        dic_voc = dict(zip(lst_voc, range(len(lst_voc))))\n",
    "        tokenizer_cds = Tokenizer(WordLevel(vocab=dic_voc, unk_token=\"[UNK]\"))\n",
    "        tokenizer_cds.add_special_tokens(['[PAD]','[CLS]', '[UNK]', '[SEP]','[MASK]'])\n",
    "        tokenizer_cds.pre_tokenizer = Whitespace()\n",
    "        tokenizer_cds.post_processor = BertProcessing(\n",
    "            (\"[SEP]\", dic_voc['[SEP]']),\n",
    "            (\"[CLS]\", dic_voc['[CLS]']),\n",
    "        )\n",
    "        # tokenizer_5utr\n",
    "        lst_voc = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        for a1 in lst_ele:\n",
    "            lst_voc.extend([f'{a1}'])\n",
    "        dic_voc = dict(zip(lst_voc, range(len(lst_voc))))\n",
    "        tokenizer_5utr = Tokenizer(WordLevel(vocab=dic_voc, unk_token=\"[UNK]\"))\n",
    "        tokenizer_5utr.add_special_tokens(['[PAD]','[CLS]', '[UNK]', '[SEP]','[MASK]'])\n",
    "        tokenizer_5utr.pre_tokenizer = Whitespace()\n",
    "        tokenizer_5utr.post_processor = BertProcessing(\n",
    "            (\"[SEP]\", dic_voc['[SEP]']),\n",
    "            (\"[CLS]\", dic_voc['[CLS]']),\n",
    "        )\n",
    "        tokenizer_3utr = tokenizer_5utr\n",
    "\n",
    "        self.tokenizer_cds = PreTrainedTokenizerFast(tokenizer_object=tokenizer_cds, \n",
    "                                                     unk_token='[UNK]',\n",
    "                                                     sep_token='[SEP]',\n",
    "                                                     pad_token='[PAD]',\n",
    "                                                     cls_token='[CLS]',\n",
    "                                                     mask_token='[MASK]')\n",
    "        self.tokenizer_5utr = PreTrainedTokenizerFast(tokenizer_object=tokenizer_5utr, \n",
    "                                                      unk_token='[UNK]',\n",
    "                                                      sep_token='[SEP]',\n",
    "                                                      pad_token='[PAD]',\n",
    "                                                      cls_token='[CLS]',\n",
    "                                                      mask_token='[MASK]')\n",
    "        self.tokenizer_3utr = PreTrainedTokenizerFast(tokenizer_object=tokenizer_3utr, \n",
    "                                                      unk_token='[UNK]',\n",
    "                                                      sep_token='[SEP]',\n",
    "                                                      pad_token='[PAD]',\n",
    "                                                      cls_token='[CLS]',\n",
    "                                                      mask_token='[MASK]')\n",
    "\n",
    "    def encode_string(self, data):\n",
    "        tok_5utr = self.tokenizer_5utr(data['5utr'], \n",
    "                                       truncation=True,  \n",
    "                                       padding=\"max_length\",\n",
    "                                       max_length=512,\n",
    "                                       return_tensors='pt')\n",
    "        tok_cds = self.tokenizer_cds(data['cds'], \n",
    "                                     truncation=True,  \n",
    "                                     padding=\"max_length\",\n",
    "                                     max_length=1024,\n",
    "                                     return_tensors='pt')\n",
    "        tok_3utr = self.tokenizer_3utr(data['3utr'], \n",
    "                                       truncation=True,  \n",
    "                                       padding=\"max_length\",\n",
    "                                       max_length=1024,\n",
    "                                       return_tensors='pt')\n",
    "\n",
    "        # Map input_ids for CDS to the filtered vocabulary\n",
    "        original_vocab_size = len(self.tokenizer_cds.get_vocab())  # 130\n",
    "        kept_indices = [i for i in range(original_vocab_size) if i not in n_indexes]  # 69 indices\n",
    "        mapping_tensor = torch.full((original_vocab_size,), 1, dtype=torch.long)  # Default to [UNK] index 1\n",
    "        for new_id, old_id in enumerate(kept_indices):\n",
    "            mapping_tensor[old_id] = new_id\n",
    "        input_ids2_mapped = mapping_tensor[tok_cds['input_ids']]\n",
    "    \n",
    "        return {\n",
    "            'input_ids1': tok_5utr['input_ids'],\n",
    "            'attention_mask1': tok_5utr['attention_mask'],\n",
    "            'input_ids2': input_ids2_mapped,\n",
    "            'attention_mask2': tok_cds['attention_mask'],\n",
    "            'input_ids3': tok_3utr['input_ids'],\n",
    "            'attention_mask3': tok_3utr['attention_mask']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264dd74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:04:52.906735Z",
     "iopub.status.busy": "2025-03-14T23:04:52.906468Z",
     "iopub.status.idle": "2025-03-14T23:05:01.428308Z",
     "shell.execute_reply": "2025-03-14T23:05:01.427485Z"
    },
    "papermill": {
     "duration": 8.526791,
     "end_time": "2025-03-14T23:05:01.429660",
     "exception": false,
     "start_time": "2025-03-14T23:04:52.902869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mRNALM model, stability fine-tuned\n",
    "model = FullModel(1, [], 32, 32, 0.5, head_dim=128)\n",
    "\n",
    "model_path = \"models/mrna_lm_saluki.bin\"\n",
    "model_dict = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6716781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:05:01.437382Z",
     "iopub.status.busy": "2025-03-14T23:05:01.437116Z",
     "iopub.status.idle": "2025-03-14T23:05:01.485651Z",
     "shell.execute_reply": "2025-03-14T23:05:01.484754Z"
    },
    "papermill": {
     "duration": 0.054003,
     "end_time": "2025-03-14T23:05:01.487224",
     "exception": false,
     "start_time": "2025-03-14T23:05:01.433221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix shape issue caused by tokenizers\n",
    "def convert_shape(tensor_voc):\n",
    "    mask = torch.ones(len(tensor_voc), dtype=torch.bool)\n",
    "    mask[torch.tensor(n_indexes)] = False\n",
    "    filtered_tensor = tensor_voc[mask]\n",
    "    return filtered_tensor\n",
    "\n",
    "\n",
    "convert_list = [\n",
    "    \"cds.base_model.model.bert.embeddings.word_embeddings.weight\",\n",
    "    \"cds.base_model.model.cls.predictions.bias\",\n",
    "    \"cds.base_model.model.cls.predictions.decoder.weight\",\n",
    "    \"cds.base_model.model.cls.predictions.decoder.bias\",\n",
    "]\n",
    "\n",
    "for i in convert_list:\n",
    "    model_dict[i] = convert_shape(model_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da5aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:05:06.227504Z",
     "iopub.status.busy": "2025-03-14T23:05:06.227255Z",
     "iopub.status.idle": "2025-03-14T23:05:06.243629Z",
     "shell.execute_reply": "2025-03-14T23:05:06.242903Z"
    },
    "papermill": {
     "duration": 0.021783,
     "end_time": "2025-03-14T23:05:06.244919",
     "exception": false,
     "start_time": "2025-03-14T23:05:06.223136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model and set to evaluation mode\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3680e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:05:06.253311Z",
     "iopub.status.busy": "2025-03-14T23:05:06.253095Z",
     "iopub.status.idle": "2025-03-14T23:05:06.647522Z",
     "shell.execute_reply": "2025-03-14T23:05:06.646706Z"
    },
    "papermill": {
     "duration": 0.399941,
     "end_time": "2025-03-14T23:05:06.648796",
     "exception": false,
     "start_time": "2025-03-14T23:05:06.248855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605da8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:05:06.658722Z",
     "iopub.status.busy": "2025-03-14T23:05:06.658418Z",
     "iopub.status.idle": "2025-03-14T23:05:09.332250Z",
     "shell.execute_reply": "2025-03-14T23:05:09.331513Z"
    },
    "papermill": {
     "duration": 2.68047,
     "end_time": "2025-03-14T23:05:09.333834",
     "exception": false,
     "start_time": "2025-03-14T23:05:06.653364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess mRNA sequence data\n",
    "DATA_PATH = \"data/human_sequence_data.csv\"\n",
    "REQUIRED_COLUMNS = [\"HALFLIFE\", \"ORF\", \"3UTR\", \"5UTR\"]\n",
    "\n",
    "sequence_dataframe = pd.read_csv(DATA_PATH, usecols=REQUIRED_COLUMNS)\n",
    "\n",
    "# Convert DNA to RNA and tokenize sequences\n",
    "processed_sequences = {\n",
    "    \"5utr\": [\n",
    "        \" \".join(list(sequence.replace(\"T\", \"U\")))\n",
    "        for sequence in sequence_dataframe[\"5UTR\"]\n",
    "    ],\n",
    "    \"cds\": [\n",
    "        \" \".join([sequence[i : i + 3] for i in range(0, len(sequence), 3)]).replace(\n",
    "            \"T\", \"U\"\n",
    "        )\n",
    "        for sequence in sequence_dataframe[\"ORF\"]\n",
    "    ],\n",
    "    \"3utr\": [\n",
    "        \" \".join(list(sequence.replace(\"T\", \"U\")))\n",
    "        for sequence in sequence_dataframe[\"3UTR\"]\n",
    "    ],\n",
    "}\n",
    "\n",
    "half_life_labels = torch.from_numpy(sequence_dataframe[\"HALFLIFE\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82869ca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:05:09.343466Z",
     "iopub.status.busy": "2025-03-14T23:05:09.343228Z",
     "iopub.status.idle": "2025-03-14T23:05:09.348016Z",
     "shell.execute_reply": "2025-03-14T23:05:09.347195Z"
    },
    "papermill": {
     "duration": 0.010717,
     "end_time": "2025-03-14T23:05:09.349148",
     "exception": false,
     "start_time": "2025-03-14T23:05:09.338431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure batch processing parameters\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "dataset_size = half_life_labels.shape[0]\n",
    "total_batches = (dataset_size + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Total batches: {total_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8a85b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T23:05:09.358524Z",
     "iopub.status.busy": "2025-03-14T23:05:09.358278Z",
     "iopub.status.idle": "2025-03-14T23:25:34.867181Z",
     "shell.execute_reply": "2025-03-14T23:25:34.866264Z"
    },
    "papermill": {
     "duration": 1225.515034,
     "end_time": "2025-03-14T23:25:34.868452",
     "exception": false,
     "start_time": "2025-03-14T23:05:09.353418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate embeddings in batches\n",
    "for batch_index in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
    "    start_index = batch_index * BATCH_SIZE\n",
    "    end_index = min(start_index + BATCH_SIZE, dataset_size)\n",
    "\n",
    "    # Extract batch sequences\n",
    "    batch_sequences = {\n",
    "        region: processed_sequences[region][start_index:end_index]\n",
    "        for region in processed_sequences.keys()\n",
    "    }\n",
    "\n",
    "    # Encode sequences and move to device\n",
    "    encoded_batch = model.encode_string(batch_sequences)\n",
    "    encoded_batch = {key: tensor.to(device) for key, tensor in encoded_batch.items()}\n",
    "    encoded_batch[\"labels\"] = half_life_labels[start_index:end_index].to(device)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        joint_embeddings, processed_hidden_states = model(\n",
    "            **encoded_batch, return_hidden=True\n",
    "        )\n",
    "\n",
    "    # Convert to numpy and save\n",
    "    joint_embeddings_np = joint_embeddings.cpu().numpy()\n",
    "    hidden_states_np = processed_hidden_states.cpu().numpy()\n",
    "\n",
    "    joint_embeddings_path = f\"mrnalm_embeddings/joint_embs/batch_{batch_index:04d}.npy\"\n",
    "    hidden_states_path = f\"mrnalm_embeddings/hidden_states/batch_{batch_index:04d}.npy\"\n",
    "    np.save(joint_embeddings_path, joint_embeddings_np)\n",
    "    np.save(hidden_states_path, hidden_states_np)\n",
    "\n",
    "    # Clean up memory\n",
    "    del joint_embeddings, processed_hidden_states, encoded_batch, batch_sequences\n",
    "    del joint_embeddings_np, hidden_states_np\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Embedding generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify generated embeddings\n",
    "sample_embeddings = np.load(\"mrnalm_embeddings/hidden_states/batch_0000.npy\")\n",
    "\n",
    "print(f\"Sample embeddings shape: {sample_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {sample_embeddings.shape[1]}\")\n",
    "print(f\"Batch size: {sample_embeddings.shape[0]}\")\n",
    "print(f\"Sample embedding norm: {np.linalg.norm(sample_embeddings[0]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76904200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python merge_batches.py -b \"mrnalm_embeddings/hidden_states/\" -o \"mrnalm_embeddings/hidden_states/merged_embeddings.npy\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5832939,
     "sourceId": 10760056,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6243565,
     "sourceId": 10777155,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6760830,
     "sourceId": 10880826,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6761106,
     "sourceId": 10881171,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6763326,
     "sourceId": 10884338,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6855587,
     "sourceId": 11011173,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1274.343463,
   "end_time": "2025-03-14T23:25:37.498075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-14T23:04:23.154612",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
