{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup directories and install dependencies\n",
        "!mkdir -p \"protbert_embeddings\"\n",
        "%pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from Bio.Seq import Seq\n",
        "import re\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ProtBERT embedding model class\n",
        "class ProtBERTEmbeddingModel:\n",
        "    def __init__(self, model_name=\"Rostlab/prot_bert\", max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "        self.model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "    \n",
        "    def translate_dna_to_protein(self, dna_sequence):\n",
        "        if not isinstance(dna_sequence, str) or len(dna_sequence) == 0:\n",
        "            return \"\"\n",
        "        \n",
        "        # Ensure sequence length is multiple of 3\n",
        "        if len(dna_sequence) % 3 != 0:\n",
        "            dna_sequence = dna_sequence[:-(len(dna_sequence) % 3)]\n",
        "        \n",
        "        # Translate sequence to protein\n",
        "        if len(dna_sequence) > 0:\n",
        "            protein_sequence = str(Seq(dna_sequence).translate())\n",
        "            # Replace unsupported amino acids with X\n",
        "            protein_sequence = re.sub(r\"[UZOB*]\", \"X\", protein_sequence)\n",
        "            return protein_sequence\n",
        "        return \"\"\n",
        "    \n",
        "    def format_protein_sequence(self, protein_sequence):\n",
        "        if not protein_sequence:\n",
        "            return \"\"\n",
        "        return \" \".join(list(protein_sequence))\n",
        "    \n",
        "    def encode_sequences(self, protein_sequences):\n",
        "        formatted_sequences = [self.format_protein_sequence(seq) for seq in protein_sequences]\n",
        "        \n",
        "        encodings = self.tokenizer(\n",
        "            formatted_sequences,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "        \n",
        "        return {key: tensor.to(self.device) for key, tensor in encodings.items()}\n",
        "    \n",
        "    def get_embeddings(self, encodings):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encodings)\n",
        "            last_hidden_states = outputs.last_hidden_state\n",
        "            attention_mask = encodings['attention_mask']\n",
        "            \n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden_states * mask_expanded, dim=1)\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "            mean_embeddings = sum_embeddings / sum_mask\n",
        "            \n",
        "            return mean_embeddings.cpu().numpy()\n",
        "    \n",
        "    def process_batch(self, protein_sequences):\n",
        "        encodings = self.encode_sequences(protein_sequences)\n",
        "        embeddings = self.get_embeddings(encodings)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ProtBERT model\n",
        "embedding_model = ProtBERTEmbeddingModel()\n",
        "print(f\"Using device: {embedding_model.device}\")\n",
        "print(f\"Model hidden size: {embedding_model.model.config.hidden_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "DATA_PATH = \"data/human_sequence_data.csv\"\n",
        "REQUIRED_COLUMNS = [\"ORF\"]\n",
        "\n",
        "sequence_dataframe = pd.read_csv(DATA_PATH, usecols=REQUIRED_COLUMNS)\n",
        "\n",
        "# Translate DNA sequences to protein sequences\n",
        "protein_sequences = [\n",
        "    embedding_model.translate_dna_to_protein(orf_sequence)\n",
        "    for orf_sequence in sequence_dataframe[\"ORF\"]\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(protein_sequences)} protein sequences\")\n",
        "print(f\"Average protein length: {np.mean([len(seq) for seq in protein_sequences if seq]):.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure batch processing parameters\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "dataset_size = len(protein_sequences)\n",
        "total_batches = (dataset_size + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "print(f\"Dataset size: {dataset_size}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Total batches: {total_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings in batches\n",
        "for batch_index in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
        "    start_index = batch_index * BATCH_SIZE\n",
        "    end_index = min(start_index + BATCH_SIZE, dataset_size)\n",
        "    \n",
        "    # Extract batch sequences\n",
        "    batch_protein_sequences = protein_sequences[start_index:end_index]\n",
        "    \n",
        "    # Generate embeddings\n",
        "    batch_embeddings = embedding_model.process_batch(batch_protein_sequences)\n",
        "    \n",
        "    # Save embeddings\n",
        "    embeddings_path = f\"protbert_embeddings/batch_{batch_index:04d}.npy\"\n",
        "    np.save(embeddings_path, batch_embeddings)\n",
        "    \n",
        "    # Clean up memory\n",
        "    del batch_embeddings, batch_protein_sequences\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Embedding generation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify generated embeddings\n",
        "sample_embeddings = np.load(\"protbert_embeddings/batch_0000.npy\")\n",
        "\n",
        "print(f\"Sample embeddings shape: {sample_embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {sample_embeddings.shape[1]}\")\n",
        "print(f\"Batch size: {sample_embeddings.shape[0]}\")\n",
        "print(f\"Sample embedding norm: {np.linalg.norm(sample_embeddings[0]):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python merge_batches.py -b \"protbert_embeddings/\" -o \"protbert_embeddings/merged_embeddings.npy\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
